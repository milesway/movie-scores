{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#load data\n",
    "trainingSet = pd.read_csv(\"train.csv\")\n",
    "testingSet = pd.read_csv(\"test.csv\")\n",
    "\n",
    "#add more features\n",
    "\n",
    "#1. add average user score and average product score\n",
    "#1.1 user average score\n",
    "train_user = pd.DataFrame(trainingSet[['UserId','Score']])\n",
    "train_user_av = train_user.groupby(['UserId']).mean().round(0).rename(columns = {\"Score\":\"average\"})\n",
    "train_user_av['average'] = train_user_av['average'].fillna(3)\n",
    "\n",
    "result = pd.merge(trainingSet, train_user_av, how=\"left\",on=\"UserId\")\n",
    "\n",
    "#1.2 product average score\n",
    "train_product = pd.DataFrame(trainingSet[['ProductId','Score']])\n",
    "train_product_av = train_product.groupby(['ProductId']).mean().round(0)\n",
    "train_product_avscore = train_product_av.rename(columns = {\"Score\":\"Product_average\"})\n",
    "\n",
    "train_product_avscore['Product_average'] = train_product_avscore['Product_average'].fillna(3)\n",
    "\n",
    "trainingSet = pd.merge(result, train_product_avscore, how=\"left\",on=\"ProductId\")\n",
    "print(trainingSet['Product_average'].isnull().sum())\n",
    "print(trainingSet['User_average'].isnull().sum())\n",
    "print(\"finish user average and product average\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. add lemma review\n",
    "#clean words with stopwords and delete all not words and tokenize\n",
    "#lemmatization\n",
    "STOPWORDS = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatization(text):\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', str(text))\n",
    "    words = text.lower().split()\n",
    "    words = [lemmatizer.lemmatize(w) for w in words if w not in STOPWORDS]\n",
    "    return ' '.join(words)\n",
    "\n",
    "trainingSet['lemma_review'] = trainingSet.Text.apply(lemmatization)\n",
    "#trainingSet['lemma_sum'] = trainingSet.Summary.apply(lemmatization)\n",
    "print(\"trainingSet lemma review\")\n",
    "print(trainingSet['lemma_review'].isnull().sum())\n",
    "print(\"finish lemma review and summary on training set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#3. add polarity\n",
    "#do sentiment test\n",
    "def sentiment(text):\n",
    "    textblob = TextBlob(text)\n",
    "    return round(textblob.polarity,3)   \n",
    "\n",
    "trainingSet['Polarity_review'] = trainingSet.lemma_review.apply(sentiment)\n",
    "#trainingSet['Polarity_sum'] = trainingSet.lemma_sum.apply(sentiment)\n",
    "print(\"finish polarity of review and summary on training set\")\n",
    "\n",
    "#create x_text and x_train\n",
    "X_test = pd.merge(trainingSet, testingSet, left_on='Id', right_on='Id')\n",
    "print(\"finish merge\")\n",
    "print(\"x_text lemma review\")\n",
    "print(X_test['lemma_review'].isnull().sum())\n",
    "\n",
    "X_test = X_test.drop(columns=['Score_x'])\n",
    "X_test = X_test.rename(columns={'Score_y': 'Score'})\n",
    "\n",
    "X_test.to_csv(\"X_submission.csv\", index=False)\n",
    "\n",
    "X_train = trainingSet[trainingSet['Score'].notnull()]\n",
    "print(\"x_train lemma review\")\n",
    "print(X_train['lemma_review'].isnull().sum())\n",
    "\n",
    "X_train.to_csv(\"X_train.csv\", index=False)\n",
    "\n",
    "# Load files into DataFrames\n",
    "X_train = pd.read_csv(\"X_train.csv\")\n",
    "X_submission = pd.read_csv(\"X_submission.csv\")\n",
    "\n",
    "# Split training set into training and testing set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X_train.drop(['Score'], axis=1),\n",
    "        X_train['Score'],\n",
    "        test_size=1/4.0,\n",
    "        random_state=0\n",
    "    )\n",
    "\n",
    "print(\"start vector\")\n",
    "print(X_train['lemma_review'].isnull().sum())\n",
    "print(X_test['lemma_review'].isnull().sum())\n",
    "print(X_submission['lemma_review'].isnull().sum())\n",
    "X_train['lemma_review'] = X_train['lemma_review'].fillna(\"n\")\n",
    "X_test['lemma_review'] = X_test['lemma_review'].fillna(\"n\")\n",
    "X_submission['lemma_review'] = X_submission['lemma_review'].fillna(\"n\")\n",
    "#4. vecotorize lemma review and summary\n",
    "#vectorize train set on review\n",
    "vectorizer_review = CountVectorizer(max_features = 5000) \n",
    "vector_review = vectorizer_review.fit_transform(X_train.lemma_review).toarray()\n",
    "#from occurance to frequencies\n",
    "#tfidf_transformer_review = TfidfTransformer()\n",
    "#train_tfidf_review = tfidf_transformer_review.fit_transform(vector_review).toarray()\n",
    "\n",
    "#vectorize test set on test review\n",
    "test_vector_review = vectorizer_review.transform(X_test.lemma_review).toarray()\n",
    "#from occurance to frequencies\n",
    "#test_tfidf_review = tfidf_transformer_review.fit(test_vector_review).toarray()\n",
    "\n",
    "#vectorize submission set on submission review\n",
    "sub_vector_review = vectorizer_review.transform(X_submission.lemma_review).toarray()\n",
    "#from occurance to frequencies\n",
    "#sub_tfidf_review = tfidf_transformer_review.fit(sub_vector_review).toarray()\n",
    "\n",
    "print(\"finish tfidf on review and summary of training set\")\n",
    "\n",
    "# Process the DataFrames\n",
    "# This is where you can do more feature extraction\n",
    "X_train_processed = X_train.drop(columns=['Id', 'ProductId', 'UserId', 'Text', 'Summary', 'lemma_review'])\n",
    "X_test_processed = X_test.drop(columns=['Id', 'ProductId', 'UserId', 'Text', 'Summary','lemma_review'])\n",
    "X_submission_processed = X_submission.drop(columns=['Id', 'ProductId', 'UserId', 'Text', 'Summary', 'Score','lemma_review'])\n",
    "\n",
    "# Learn the review model\n",
    "#create random forest model\n",
    "forest_review = RandomForestClassifier(n_estimators = 100,n_jobs=10)\n",
    "model_review = forest_review.fit(vector_review, Y_train)\n",
    "print(\"finish review fit\")\n",
    "\n",
    "# Learn the model\n",
    "#create random forest model\n",
    "forest = RandomForestClassifier(n_estimators = 100,n_jobs=10)\n",
    "model = forest.fit(X_train_processed, Y_train)\n",
    "print(\"finish regular fit\")\n",
    "\n",
    "# Predict based on review\n",
    "Y_test_review_pre = model_review.predict(test_vector_review)\n",
    "X_submission['Score1'] = model_review.predict(sub_vector_review)\n",
    "\n",
    "# Predict the score using the model\n",
    "Y_test_regular_pre = model.predict(X_test_processed)\n",
    "X_submission['Score2'] = model.predict(X_submission_processed)\n",
    "\n",
    "Y_av = (Y_test_review_pre * 0.5 + Y_test_regular_pre * 0.5).round(0)\n",
    "X_submission['Score']=(X_submission['Score1']*0.5+X_submission['Score2']*0.5).round(0)\n",
    "\n",
    "Y_av = (Y_test_review_pre * 0.5 + Y_test_regular_pre * 0.5).round(0)\n",
    "X_submission['Score']=(X_submission['Score1']*0.5+X_submission['Score2']*0.5).round(0)\n",
    "\n",
    "# Evaluate your model on the testing set\n",
    "print(\"RMSE on testing set: review predict = \", mean_squared_error(Y_test, Y_test_review_pre))\n",
    "print(\"RMSE on testing set: regular predict = \", mean_squared_error(Y_test, Y_test_regular_pre))\n",
    "print(\"RMSE on testing set: regular predict = \", mean_squared_error(Y_test, Y_av))\n",
    "\n",
    "# Plot a confusion matrix\n",
    "#cm1 = confusion_matrix(Y_test, Y_test_review_pre, normalize='true')\n",
    "#print(cm1)\n",
    "#cm2 = confusion_matrix(Y_test, Y_test_regular_pre, normalize='true')\n",
    "#print(cm2)\n",
    "#sns.heatmap(cm, annot=True)\n",
    "#plt.title('Confusion matrix of the classifier')\n",
    "#plt.xlabel('Predicted')\n",
    "#plt.ylabel('True')\n",
    "#plt.show()\n",
    "\n",
    "# Create the submission file\n",
    "submission = X_submission[['Id', 'Score1']]\n",
    "submission.to_csv(\"submission1.csv\", index=False)\n",
    "\n",
    "submission = X_submission[['Id', 'Score2']]\n",
    "submission.to_csv(\"submission2.csv\", index=False)\n",
    "\n",
    "submission = X_submission[['Id', 'Score']]\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#load data\n",
    "trainingSet = pd.read_csv(\"submission_2.csv\")\n",
    "\n",
    "testingSet = pd.read_csv(\"submission_4.csv\")\n",
    "\n",
    "X_submission = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_submission['Score']=(trainingSet['Score']*90+testingSet['Score']*10)//100\n",
    "\n",
    "submission = X_submission[['Id', 'Score']]\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125809.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_submission['Score'][:30000].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
