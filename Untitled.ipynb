{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "finish user average and product average\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'fillnan'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e9257435787a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mtrainingSet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lemma_review'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainingSet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mText\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatization\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;31m#trainingSet['lemma_sum'] = trainingSet.Summary.apply(lemmatization)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"finish lemma review and summary on training set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5065\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5066\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5067\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5069\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'fillnan'"
     ]
    }
   ],
   "source": [
    "# Import Packages\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#load data\n",
    "trainingSet = pd.read_csv(\"train.csv\")\n",
    "testingSet = pd.read_csv(\"test.csv\")\n",
    "\n",
    "#add more features\n",
    "\n",
    "#1. add average user score and average product score\n",
    "#1.1 user average score\n",
    "train_user = pd.DataFrame(trainingSet[['UserId','Score']])\n",
    "train_user_av = train_user.groupby(['UserId']).mean().round(0)\n",
    "train_user_avscore = train_user_av.rename(columns = {\"Score\":\"User_average\"})\n",
    "\n",
    "train_user_avscore['User_average'] = train_user_avscore['User_average'].fillna(3)\n",
    "\n",
    "result = pd.merge(trainingSet, train_user_avscore, how=\"left\",on=\"UserId\")\n",
    "\n",
    "#1.2 product average score\n",
    "train_product = pd.DataFrame(trainingSet[['ProductId','Score']])\n",
    "train_product_av = train_product.groupby(['ProductId']).mean().round(0)\n",
    "train_product_avscore = train_product_av.rename(columns = {\"Score\":\"Product_average\"})\n",
    "\n",
    "train_product_avscore['Product_average'] = train_product_avscore['Product_average'].fillna(3)\n",
    "\n",
    "trainingSet = pd.merge(result, train_product_avscore, how=\"left\",on=\"ProductId\")\n",
    "print(trainingSet['Product_average'].isnull().sum())\n",
    "print(trainingSet['User_average'].isnull().sum())\n",
    "print(\"finish user average and product average\")\n",
    "\n",
    "#2. add lemma review\n",
    "#clean words with stopwords and delete all not words and tokenize\n",
    "#lemmatization\n",
    "STOPWORDS = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatization(text):\n",
    "    # textblob = TextBlob(text)\n",
    "    # textblob.correct()\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', str(text))\n",
    "    words = text.lower().split()\n",
    "    words = [lemmatizer.lemmatize(w) for w in words if w not in STOPWORDS]\n",
    "    return ' '.join(words)\n",
    "\n",
    "trainingSet['lemma_review'] = trainingSet.Text.apply(lemmatization).fillnan(\" \")\n",
    "#trainingSet['lemma_sum'] = trainingSet.Summary.apply(lemmatization)\n",
    "print(\"finish lemma review and summary on training set\")\n",
    "\n",
    "#3. add polarity\n",
    "#do sentiment test\n",
    "def sentiment(text):\n",
    "    textblob = TextBlob(text)\n",
    "    return textblob.polarity    \n",
    "\n",
    "trainingSet['Polarity_review'] = trainingSet.lemma_review.apply(sentiment)\n",
    "#trainingSet['Polarity_sum'] = trainingSet.lemma_sum.apply(sentiment)\n",
    "print(\"finish polarity of review and summary on training set\")\n",
    "\n",
    "#create x_text and x_train\n",
    "X_test = pd.merge(trainingSet, testingSet, left_on='Id', right_on='Id')\n",
    "\n",
    "X_test = X_test.drop(columns=['Score_x'])\n",
    "X_test = X_test.rename(columns={'Score_y': 'Score'})\n",
    "\n",
    "X_test.to_csv(\"X_submission.csv\", index=False)\n",
    "\n",
    "X_train = trainingSet[trainingSet['Score'].notnull()]\n",
    "\n",
    "X_train.to_csv(\"X_train.csv\", index=False)\n",
    "\n",
    "# Load files into DataFrames\n",
    "X_train = pd.read_csv(\"X_train.csv\")\n",
    "X_submission = pd.read_csv(\"X_submission.csv\")\n",
    "\n",
    "# Split training set into training and testing set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X_train.drop(['Score'], axis=1),\n",
    "        X_train['Score'],\n",
    "        test_size=1/4.0,\n",
    "        random_state=0\n",
    "    )\n",
    "\n",
    "#4. vecotorize lemma review and summary\n",
    "#vectorize train set on review\n",
    "vectorizer_review = CountVectorizer(max_features = 1000) \n",
    "vector_review = vectorizer_review.fit_transform(X_train.lemma_review)\n",
    "#from occurance to frequencies\n",
    "tfidf_transformer_review = TfidfTransformer()\n",
    "train_tfidf_review = tfidf_transformer_review.fit_transform(vector_review)\n",
    "\n",
    "#vectorize test set on test review\n",
    "test_vector_review = vectorizer_review.fit(X_test.lemma_review)\n",
    "#from occurance to frequencies\n",
    "test_tfidf_review = tfidf_transformer_review.fit(test_vector_review)\n",
    "\n",
    "#vectorize submission set on submission review\n",
    "sub_vector_review = vectorizer_review.fit(X_submission.lemma_review)\n",
    "#from occurance to frequencies\n",
    "sub_tfidf_review = tfidf_transformer_review.fit(sub_vector_review)\n",
    "\n",
    "print(\"finish tfidf on review and summary of training set\")\n",
    "\n",
    "# Process the DataFrames\n",
    "# This is where you can do more feature extraction\n",
    "X_train_processed = X_train.drop(columns=['Id', 'ProductId', 'UserId', 'Text', 'Summary', 'lemma_review'])\n",
    "X_test_processed = X_test.drop(columns=['Id', 'ProductId', 'UserId', 'Text', 'Summary','lemma_review'])\n",
    "X_submission_processed = X_submission.drop(columns=['Id', 'ProductId', 'UserId', 'Text', 'Summary', 'Score','lemma_review'])\n",
    "\n",
    "# Learn the review model\n",
    "#create random forest model\n",
    "forest_review = RandomForestClassifier(n_estimators = 100)\n",
    "model_review = forest_review.fit(train_tfidf_review, Y_train)\n",
    "print(\"finish review fit\")\n",
    "\n",
    "# Learn the model\n",
    "#create random forest model\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "model = forest.fit(X_train_processed, Y_train)\n",
    "print(\"finish regular fit\")\n",
    "\n",
    "# Predict based on review\n",
    "Y_test_review_pre = model_review.predict(test_tfidf_review)\n",
    "X_submission['Score1'] = model_review.predict(sub_tfidf_review)\n",
    "\n",
    "# Predict the score using the model\n",
    "Y_test_regular_pre = model.predict(X_test_processed)\n",
    "X_submission['Score2'] = model.predict(X_submission_processed)\n",
    "\n",
    "Y_av = (Y_test_review_pre * 0.5 + Y_test_regular_pre * 0.5).round(0)\n",
    "X_submission['Score']=(X_submission['Score1']*0.5+X_submission['Score2']*0.5).round(0)\n",
    "\n",
    "# Evaluate your model on the testing set\n",
    "print(\"RMSE on testing set: review predict = \", mean_squared_error(Y_test, Y_test_review_pre))\n",
    "print(\"RMSE on testing set: regular predict = \", mean_squared_error(Y_test, Y_test_regular_pre))\n",
    "print(\"RMSE on testing set: regular predict = \", mean_squared_error(Y_test, Y_av))\n",
    "\n",
    "# Plot a confusion matrix\n",
    "#cm1 = confusion_matrix(Y_test, Y_test_review_pre, normalize='true')\n",
    "#print(cm1)\n",
    "#cm2 = confusion_matrix(Y_test, Y_test_regular_pre, normalize='true')\n",
    "#print(cm2)\n",
    "#sns.heatmap(cm, annot=True)\n",
    "#plt.title('Confusion matrix of the classifier')\n",
    "#plt.xlabel('Predicted')\n",
    "#plt.ylabel('True')\n",
    "#plt.show()\n",
    "\n",
    "# Create the submission file\n",
    "submission = X_submission[['Id', 'Score1']]\n",
    "submission.to_csv(\"submission1.csv\", index=False)\n",
    "\n",
    "submission = X_submission[['Id', 'Score2']]\n",
    "submission.to_csv(\"submission2.csv\", index=False)\n",
    "\n",
    "submission = X_submission[['Id', 'Score']]\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
